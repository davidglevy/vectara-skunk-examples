{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d79e3b4-ceab-4021-9721-f48a22c4a134",
   "metadata": {},
   "source": [
    "# 05 - Prompt Engineering\n",
    "We'll now exlore the possibilities with prompt engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b5eb6-854e-4b4a-b5aa-423796cfac61",
   "metadata": {},
   "source": [
    "## Lab Setup\n",
    "We'll setup our lab and use the public reports from a few Australia public sector agencies for our corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e796428c-0fa1-47d0-b5a9-67f7ba11f32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q vectara-skunk-client==0.4.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a3d0be2-2439-49b2-bdef-8a158710f0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:59:42 +1100 lab_setup            INFO:User prefix for lab: david\n",
      "08:59:42 +1100 lab_setup            INFO:Setting up lab corpus with name [david-05-lab-prompt-engineering]\n",
      "08:59:42 +1100 Factory              INFO:initializing builder\n",
      "08:59:42 +1100 Factory              INFO:Factory will load configuration from home directory\n",
      "08:59:42 +1100 root                 INFO:We are processing authentication type [OAuth2]\n",
      "08:59:42 +1100 root                 INFO:initializing Client\n",
      "08:59:44 +1100 root                 INFO:No existing corpus with the name david-05-lab-prompt-engineering\n",
      "08:59:46 +1100 AdminService         INFO:Created new corpus with 246\n",
      "08:59:46 +1100 root                 INFO:New corpus created CreateCorpusResponse(corpusId=246, status=Status(code=<StatusCode.OK: 0>, statusDetail='Corpus Created', cause=None))\n",
      "08:59:46 +1100 lab_setup            INFO:New lab created with id [246]\n"
     ]
    }
   ],
   "source": [
    "from lab_setup import create_lab_corpus\n",
    "corpus_id = create_lab_corpus(\"05-lab-prompt-engineering\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90e7be61-9f05-4660-b623-4c360af95036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:59:47 +1100 Factory              INFO:initializing builder\n",
      "08:59:47 +1100 Factory              INFO:Factory will load configuration from home directory\n",
      "08:59:47 +1100 root                 INFO:We are processing authentication type [OAuth2]\n",
      "08:59:47 +1100 root                 INFO:initializing Client\n",
      "08:59:47 +1100 IndexerService       INFO:Headers: {\"c\": \"1623270172\", \"o\": \"246\"}\n",
      "Avoiding Hallucinations in LLMs.pdf: 3.31MB [00:05, 665kB/s]                                                         \n",
      "08:59:54 +1100 IndexerService       INFO:Headers: {\"c\": \"1623270172\", \"o\": \"246\"}\n",
      "RAG done right - part 1 - chunking.pdf: 4.69MB [00:05, 831kB/s]                                                      \n",
      "09:00:00 +1100 IndexerService       INFO:Headers: {\"c\": \"1623270172\", \"o\": \"246\"}\n",
      "RAG done right - part 2 - retrieval.pdf: 3.32MB [00:05, 618kB/s]                                                     \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from vectara.client.core import Factory\n",
    "\n",
    "resources_dir = Path(\"./resources/05_prompt_engineering/vectara\")\n",
    "client = Factory().build()\n",
    "indexer_service = client.indexer_service\n",
    "\n",
    "for p in resources_dir.glob(\"*.pdf\"):\n",
    "    indexer_service.upload(corpus_id, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5604d9e-bf8a-47fe-84a1-695e7d1f18c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectara.client.core import Factory\n",
    "\n",
    "client = Factory().build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af4804f-14de-4524-a86f-02de7dfac254",
   "metadata": {},
   "source": [
    "## Summarization Using Default Prompt\n",
    "The query below uses our default summarizer (vectara-summary-ext-v1.2.0 aka GPT 3.5) to return the results using RAG only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b68b06d-dd96-4e83-aa29-248fc8f7622d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# Query: Why is Vectara's platform better than other LLM options?\n",
       "\n",
       "The returned results did not contain sufficient information to be summarized into a useful answer for your query. Please try a different search or restate your query differently.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lab_setup import render_response\n",
    "\n",
    "query = \"Why is Vectara's platform better than other LLM options?\"\n",
    "qs = client.query_service\n",
    "resp = qs.query(query, corpus_id)\n",
    "render_response(query, resp, show_search_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df64d70-8334-433f-a5b1-38b118a63adc",
   "metadata": {},
   "source": [
    "## Summarization Using Default Prompt\n",
    "We're now creating a custom prompt with the following preferences:\n",
    "\n",
    "1. We want to tailor the style by setting `persona=\"Head of Investor Relations\"`\n",
    "2. We show a more concise answer instead of citing each result by setting `cite=False`\n",
    "3. We will also allow _general_context_ to creep in from the base LLM by setting the `just_rag=False` . Whilst this may increase the risk of hallucinations, if done right it can bolster the results from the retrieval model.\n",
    "4. We create a more concise result by setting `max_word_count=100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d78ff8d-8965-480f-b078-84568d34ee70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09:00:47 +1100 root                 INFO:Here's our tailored \"promptText\" we'll be supplying on the query:\n",
      "\n",
      "[ {\"role\": \"system\", \"content\": \"You are a Head of Investor Relations who takes the search results and only return the most relevant answer. Do not iterate over each question, preferably based on the search results in this chat. You may allow additional information you know in the results. Respond in the language denoted by ISO 639 code \\\"$vectaraLangCode\\\".\"}, \n",
      "#foreach ($qResult in $vectaraQueryResults) \n",
      "   #if ($foreach.first) \n",
      "   {\"role\": \"user\", \"content\": \"Search for \\\"$esc.java(${vectaraQuery})\\\", and give me the first search result.\"}, \n",
      "   {\"role\": \"assistant\", \"content\": \"$esc.java(${qResult.getText()})\" }, \n",
      "   #else \n",
      "   {\"role\": \"user\", \"content\": \"Give me the \\\"$vectaraIdxWord[$foreach.index]\\\" search result.\"}, \n",
      "   {\"role\": \"assistant\", \"content\": \"$esc.java(${qResult.getText()})\" }, \n",
      "   #end \n",
      " #end \n",
      "{\"role\": \"user\", \"content\": \"Generate a detailed answer (that is no more than 100 words) for the query \\\"$esc.java(${vectaraQuery})\\\" preferably based on the search results in this chat. You may allow additional information you know in the results. Do not cite search results.\" } ]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "# Query: Why is Vectara's platform better than other LLM options?\n",
       "\n",
       "Vectara's platform stands out amongst other Language Model options due to its advanced Retrieval Augmented Generation (RAG) capabilities. Unlike traditional models, Vectara's RAG optimizes information retrieval, ensuring more accurate and contextually relevant responses. Moreover, Vectara's unique \"chunking\" feature helps break down complex queries into manageable parts for better comprehension and response generation. This combination of advanced retrieval and processing capabilities makes Vectara's platform a superior choice.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vectara.client.util import SimplePromptFactory\n",
    "import logging\n",
    "\n",
    "prompt_factory = SimplePromptFactory(persona=\"Head of Investor Relations\", cite=False, max_word_count=100, just_rag=False)\n",
    "prompt_text = prompt_factory.build()\n",
    "\n",
    "logging.info(f\"Here's our tailored \\\"promptText\\\" we'll be supplying on the query:\\n\\n{prompt_text}\\n\")\n",
    "\n",
    "resp = qs.query(query, corpus_id, promptText=prompt_text)\n",
    "render_response(query, resp, show_search_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8512aaca-410b-49c5-b5a6-56f2ed9f8644",
   "metadata": {},
   "source": [
    "## Persona: ELI5\n",
    "Now lets respond as though we're doing it using \"Explain Like I'm 5\" language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5202af4-d659-4681-b443-7e235d3e0b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# Query: Why is Vectara's platform better than other LLM options?\n",
       "\n",
       "Vectara's platform stands out among other Language Model options due to its unique features. It employs techniques like avoiding hallucinations in applications, which ensures more accurate and reliable outputs. Additionally, it uses Retrieval Augmented Generation (RAG) effectively, breaking information into manageable chunks for better comprehension. This makes it highly efficient and accurate, enhancing the overall user experience.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_factory = SimplePromptFactory(persona=\"Explaning to someone in ELI5 language\", cite=False, max_word_count=100, just_rag=False)\n",
    "prompt_text = prompt_factory.build()\n",
    "resp = qs.query(query, corpus_id, promptText=prompt_text)\n",
    "render_response(query, resp, show_search_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13a9ee7b-a529-46f3-b7e0-d620699b3607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# Query: Explain what RAG is?\n",
       "\n",
       "I'm sorry, but the search results provided do not contain enough information to accurately and comprehensively explain what RAG (Retrieval Augmented Generation) is. The titles suggest that RAG relates to something being 'done right', possibly in relation to 'chunking' and 'retrieval' [1,2], and it might be used in applications to avoid 'hallucinations' [3]. However, without more detailed information, a comprehensive explanation of RAG is not possible.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Explain what RAG is?\"\n",
    "prompt_factory = SimplePromptFactory(persona=\"Explaning to someone in ELI5 language\", cite=True, max_word_count=100, just_rag=True)\n",
    "prompt_text = prompt_factory.build()\n",
    "resp = qs.query(query, corpus_id, promptText=prompt_text)\n",
    "render_response(query, resp, show_search_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b527d62-22d5-4104-8f9c-c88ae716b8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# Query: What is steam power?\n",
       "\n",
       "I apologize for the confusion, but the search results provided do not contain information about steam power. However, I can tell you that steam power is a type of power that is generated by heating water until it becomes steam. The steam then drives a steam turbine or engine, which produces mechanical power. This mechanical power can be used for various applications, such as powering machinery or generating electricity.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"What is steam power?\"\n",
    "prompt_factory = SimplePromptFactory(persona=\"Explaning to someone in ELI5 language\", cite=True, max_word_count=100, just_rag=False)\n",
    "prompt_text = prompt_factory.build()\n",
    "resp = qs.query(query, corpus_id, promptText=prompt_text)\n",
    "render_response(query, resp, show_search_results=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
