{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "668e7f3a-16be-42e0-893c-ac345844a3af",
   "metadata": {},
   "source": [
    "# Vectara-Skunk-Client Example\n",
    "In the code below we show how easy it is to get started with the client. Before these steps, please ensure you have:\n",
    "1. Generate either an API Key or OAuth2 App from within Vectara's console.\n",
    "2. Put these into a configuration \".vec_auth.yaml\" in your home directory\n",
    "\n",
    "Format for configuration should should match the following:\n",
    "```yaml\n",
    "default:\n",
    "  customer_id : \"1999999999\"\n",
    "  auth:\n",
    "      # For API Key, you only need the API key\n",
    "      api_key : \"abcdabcdabcdabcdabcdabcdababcdabcd\"\n",
    "admin:\n",
    "  customer_id : \"1999999999\" # Customer Id as a string\n",
    "  # For OAuth2, you need app_client_id, app_client_secret, auth_url\n",
    "  auth:\n",
    "      app_client_id : \"abcdabcdabcdabcdabcdabcdab\"\n",
    "      app_client_secret : \"abcdabcdabcdabcdabcdabcdababcdabcdabcdabcdabcdabcdab\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e782618-04c9-4334-820d-f75b9b86e99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q vectara-skunk-client==0.4.11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22d04a1-0236-469b-8e85-e7c5928f38bd",
   "metadata": {},
   "source": [
    "## Setup Logging\n",
    "The Client has extensive logging but we need to make sure it's activated within our Python environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "419570be-b4c2-486b-b28a-e5db17f8e91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(name)-20s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%H:%M:%S %z')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7597f3b2-f359-41d2-8840-0ab8754c7afb",
   "metadata": {},
   "source": [
    "## Create the Client Factory\n",
    "In the code below we create the client factory without any arguments, this will then use the configuration file in the users home directory.\n",
    "\n",
    "The full factory flow is shown here:\n",
    "\n",
    "<img src=\"https://github.com/davidglevy/vectara-skunk-client/raw/main/resources/images/factory-build-flow.png\" alt=\"Factory Build Flow\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7d7131f-a3ea-4a76-9a51-a63f2c4ddaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:53:18 +1100 Factory              INFO:initializing builder\n",
      "08:53:18 +1100 Factory              INFO:Factory will load configuration from home directory\n",
      "08:53:18 +1100 HomeConfigLoader     INFO:Loading configuration from users home directory [C:\\Users\\david]\n",
      "08:53:18 +1100 HomeConfigLoader     INFO:Loading default configuration [default]\n",
      "08:53:18 +1100 HomeConfigLoader     INFO:Parsing config\n",
      "08:53:18 +1100 root                 INFO:We are processing authentication type [OAuth2]\n",
      "08:53:18 +1100 OAuthUtil            INFO:Using provided OAuth2 URL [https://vectara-prod-1623270172.auth.us-west-2.amazoncognito.com/oauth2/token]\n",
      "08:53:18 +1100 OAuthUtil            INFO:OAuth2 URL is [https://vectara-prod-1623270172.auth.us-west-2.amazoncognito.com/oauth2/token]\n",
      "08:53:18 +1100 root                 INFO:initializing Client\n"
     ]
    }
   ],
   "source": [
    "from vectara.client.core import Factory\n",
    "\n",
    "client = Factory().build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "308b6a64-497a-4188-b82b-6bbd62290c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:53:23 +1100 OAuthUtil            INFO:Current timestamp 2024-01-13 08:53:23.344876\n",
      "08:53:23 +1100 OAuthUtil            INFO:First time requesting token, authenticating\n",
      "08:53:24 +1100 OAuthUtil            INFO:Received OAuth token, will expire [01/13/2024, 09:53:24]\n",
      "08:53:24 +1100 OAuthUtil            INFO:Already authenticated with non-expired token, expiry is [1705100004]\n",
      "08:53:24 +1100 RequestUtil          INFO:URL for operation list-corpora is: https://api.vectara.io/v1/list-corpora\n",
      "08:53:26 +1100 root                 INFO:Found corpus [Australia Broadband] with id [6]\n",
      "08:53:26 +1100 root                 INFO:Found corpus [Australian Importation Laws] with id [10]\n",
      "08:53:26 +1100 root                 INFO:Found corpus [High Court of Australia] with id [17]\n",
      "08:53:26 +1100 root                 INFO:Found corpus [Public Reports] with id [8]\n",
      "08:53:26 +1100 root                 INFO:Found corpus [SE] with id [7]\n",
      "08:53:26 +1100 root                 INFO:Found corpus [South Australian State Law] with id [18]\n",
      "08:53:26 +1100 root                 INFO:Found corpus [Talabat] with id [177]\n",
      "08:53:26 +1100 root                 INFO:Found corpus [david.BasicQuery.test_run_basic_query] with id [172]\n",
      "08:53:26 +1100 root                 INFO:Found corpus [david.FilterAttributeTest.test_filter_attr] with id [176]\n",
      "08:53:26 +1100 root                 INFO:Found corpus [david.FilterAttributes.test_index_doc] with id [174]\n"
     ]
    }
   ],
   "source": [
    "# Quick test to verify we can see all corpora\n",
    "admin_service = client.admin_service\n",
    "corpora = admin_service.list_corpora()\n",
    "for corpus in corpora:\n",
    "    logging.info(f\"Found corpus [{corpus.name}] with id [{corpus.id}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cef01b-7f6d-41c7-b5bd-cd71cb816b45",
   "metadata": {},
   "source": [
    "## Check for Existing Corpus\n",
    "We'll now check if our test corpus exists, and if so, delete it so we start with a clean slate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8e95755-d3d6-4e47-84c2-886a0f42c1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:53:28 +1100 OAuthUtil            INFO:Current timestamp 2024-01-13 08:53:28.475589\n",
      "08:53:28 +1100 OAuthUtil            INFO:Expiry            2024-01-13 09:53:24\n",
      "08:53:28 +1100 OAuthUtil            INFO:Already authenticated with non-expired token, expiry is [1705100004]\n",
      "08:53:28 +1100 RequestUtil          INFO:URL for operation list-corpora is: https://api.vectara.io/v1/list-corpora\n",
      "08:53:29 +1100 root                 INFO:No existing corpus with the name client-example\n"
     ]
    }
   ],
   "source": [
    "admin_service = client.admin_service\n",
    "\n",
    "corpora = admin_service.list_corpora(\"01-first-client-example\")\n",
    "\n",
    "if len(corpora) >= 1:\n",
    "    for corpus in corpora:\n",
    "        admin_service.delete_corpus(corpus.id)\n",
    "else:\n",
    "    logging.info(\"No existing corpus with the name client-example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cbb737-0f68-4f41-87b5-75ba3be38aad",
   "metadata": {},
   "source": [
    "## Create New Corpus\n",
    "We now use the simple signature vectara.admin.AdminService#create_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16314ed1-41c9-4aac-9d70-275d930af975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:53:31 +1100 OAuthUtil            INFO:Current timestamp 2024-01-13 08:53:31.509449\n",
      "08:53:31 +1100 OAuthUtil            INFO:Expiry            2024-01-13 09:53:24\n",
      "08:53:31 +1100 OAuthUtil            INFO:Already authenticated with non-expired token, expiry is [1705100004]\n",
      "08:53:31 +1100 RequestUtil          INFO:URL for operation create-corpus is: https://api.vectara.io/v1/create-corpus\n",
      "08:53:33 +1100 AdminService         INFO:Created new corpus with 242\n",
      "08:53:33 +1100 root                 INFO:New corpus created CreateCorpusResponse(corpusId=242, status=Status(code=<StatusCode.OK: 0>, statusDetail='Corpus Created', cause=None))\n"
     ]
    }
   ],
   "source": [
    "create_corpus_result = admin_service.create_corpus(\"01-first-client-example\", description=\"Example Corpus for use from Jupyter\")\n",
    "logging.info(f\"New corpus created {create_corpus_result}\")\n",
    "corpus_id = create_corpus_result.corpusId"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdeefed-a0eb-4edf-873e-fb1f03958ff5",
   "metadata": {},
   "source": [
    "## Load the Corpus\n",
    "We'll now load the corpus using a file on our computer. This file is a word document which will be automatically parsed by Vectara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "139152b5-5bda-4fec-8fb1-1b2ac97ccd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:53:33 +1100 IndexerService       INFO:Headers: {\"c\": \"1623270172\", \"o\": \"242\"}\n",
      "08:53:33 +1100 OAuthUtil            INFO:Current timestamp 2024-01-13 08:53:33.728835\n",
      "08:53:33 +1100 OAuthUtil            INFO:Expiry            2024-01-13 09:53:24\n",
      "08:53:33 +1100 OAuthUtil            INFO:Already authenticated with non-expired token, expiry is [1705100004]\n",
      "RAG Options.docx: 695kB [00:04, 163kB/s]                                                                             \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UploadDocumentResponse(response=UploadDocumentResponseInner(status=None, quotaConsumed=StorageQuota(numChars='7936', numMetadataChars='3592')), document=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer_service = client.indexer_service\n",
    "indexer_service.upload(corpus_id, \"C:\\\\Users\\\\david\\\\OneDrive\\\\Documents\\\\Publications\\\\RAG Options.docx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4c51e6-d4bd-4fd7-a676-04b6f4f2744b",
   "metadata": {},
   "source": [
    "## Query the Corpus\n",
    "Lets run a basic query on the corpus with only a single document in it - answers improve with more relevant material but this shows the complete loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be48d340-e383-4732-a8a8-f4d330e0f32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:53:40 +1100 OAuthUtil            INFO:Current timestamp 2024-01-13 08:53:40.706530\n",
      "08:53:40 +1100 OAuthUtil            INFO:Expiry            2024-01-13 09:53:24\n",
      "08:53:40 +1100 OAuthUtil            INFO:Already authenticated with non-expired token, expiry is [1705100004]\n",
      "08:53:40 +1100 RequestUtil          INFO:URL for operation query is: https://api.vectara.io/v1/query\n"
     ]
    }
   ],
   "source": [
    "query_service = client.query_service\n",
    "query = \"Why is Vectara a good RAG system?\"\n",
    "response = query_service.query(query, corpus_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c449680f-92d8-4d50-b14a-ecbd379e8444",
   "metadata": {},
   "source": [
    "## Show Result in Markdown\n",
    "We'll now use the utility method \"renderMarkdown\" to render the Python dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32830643-90d8-4b2e-b1c2-955a27d3d30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "# Query: Why is Vectara a good RAG system?\n",
       "\n",
       "Vectara is a good Retrieval Augmented Generation (RAG) system because it is designed specifically to cater to \"SaaS for serving RAG operationally,\" making it a ready-to-use system for these workloads [1]. Developed with a focus on application builders, it provides an easy path to Generative AI solutions [5]. Vectara's RAG features are considered to be on another level and it aims to be the \"Snowflake of RAG,\" ensuring an optimal level of abstraction for developers coming from different platforms [3]. It was established when RAG was a less explored space, so it has a first-mover advantage in this field [2]. Lastly, its advanced features such as Access Control Lists (ACLs) and multi-corpus searches set it apart from other RAG systems [4].\n",
       "\n",
       " 1. · Multi-Corpus Queries? · ACLs on the Corpus? <b>Scoring Category Through the lens of “SaaS specifically for serving RAG operationally”, Vectara is the only ready-to-go system for serving these workloads.</b> The rest are either “platforms that can be used for” or “libraries” rather than a capability. No surprise, Vectara scored highest here. *score: 0.93919635, doc-id: RAG Options.docx*\n",
       " 2. RAG Options What the thing is: Choosing a RAG in today’s market Options · Cohere · OpenAI · Azure Search · Google AI · LangChain · Llama Index · Databricks Summarise · Background Sentence · Who each tech is created for (App Builders | Data Engineers | DSs) · Cost model (scale to zero) · Abstraction Strengths/Weaknesses · Ease of Use (right abstraction etc) · Operational Cost · Trust · Model Coupling · Advanced Features (Hybrid) Conclusion Introduction Wow what a difference a day makes!! <b>When Vectara was founded the Retrieval Augmented Generation (RAG) was a very lonely space.</b> But from the explosion of LLMs in 2022 have also seen the number of solutions which propose a RAG element to also increase. With this document, we aim to look at who are the major players in this space and do a feature comparison. *score: 0.844621, doc-id: RAG Options.docx*\n",
       " 3. You could achieve Data Warehousing with Databricks in 2021 but there was a lot of banging, tears and a fair few sharp corners to get something equivalent to Snowflake. Databricks bridged that gap with Serverless and Unity Catalog, but kudos where kudos is warranted, Snowflake is still the optimum level of abstraction for many Data Warehouse folk coming from 20 years on legacy platforms. <b>Vectara’s goal is to be the Snowflake of RAG, being the equivalent for Application Builders.</b> Team Costs How many cooks does it take to get your RAG pipeline working … and to continue working. This measure is subjective but based on my experiences working with technical teams across my career. *score: 0.84389603, doc-id: RAG Options.docx*\n",
       " 4. The Players Like the introduction of the tributes in “The Hunger Games” here are the contenders. Amr would have been great representing District 12!! <b>Vectara Founded in 2019 which has been designed from the ground up to be a focused RAG solution for application builders with easy APIs.</b> The “Snowflake” of the category – purpose built to allow organisations to think about their IP in terms of Corpus with advanced features like ACLs and multi-corpus searches. Cohere Also founded in 2019 with multiple researchers from “Google Brain”, one of which was the author of the famous “Attention is All you Need” paper which introduced the transformer architecture and lay the groundwork for other heavy weights below. *score: 0.83304876, doc-id: RAG Options.docx*\n",
       " 5. Fifth: Azure Search (21.2) Conclusion I mean, we’re tooting our own horn and came out first? Who would have thought. <b>But seriously though: Vectara’s RAG features are on another level and the focused delivery of a platform for application builders by application builders has resulted in an incredibly easy path to Generative AI solutions.</b> image1.png *score: 0.8089236, doc-id: RAG Options.docx*\n",
       " 6. Not what I intended at all. https://twitter.com/elonmusk/status/1626516035863212034 All that being said, they are the leaders in terms of General LLMs and have made moves towards RAG with their Assistant feature. <b>Azure Search This is a “build it yourself” RAG system – achievable but you own the most of the steps and leveraging Open AI’s features to their Cognitive Search.</b> I mean, at least they didn’t completely rip off one of their Open Source partners tech stacks and rebadge it as their own this time. Google AI TBD LangChain TBD Llama Index TBD Databricks I would be remiss not to also mention Databricks as a former employer. *score: 0.7482908, doc-id: RAG Options.docx*\n",
       " 7. No surprise, Vectara scored highest here. Completeness In the realm of “Don’t make me think”, how much of each platform is done for our users? <b>Again Vectara is platform that “just works” whereas others require many of the additional steps to be both done and wired in.</b> Again, Vecata scores well here. For example, Cohere is mature but you have to “do-it-yourself” to accomplish the major parts of RAG (parsing, encoding, vector storage and search): https://docs.cohere.com/docs/retrieval-augmented-generation-rag#step-2-fetching-relevant-documents Vendor\tComment by David Levy: Justin: Could use your expertise with Google AI, LangChain and LlamaIndex Parsing Encoding Vector Storage Retrieval Prompt Engineering LLM Execution Score\tComment by David Levy: I have a sheet to calculate scores - will update after we put in Google AI, LangChain and LlamaIndex Vectara Yes Yes Yes Yes Yes Yes 10.0 Cohere No No No No Yes Yes 3.3 OpenAI Yes Yes Yes Partial Yes Yes 9.2 Azure Search* No No Yes Partial Partial Partial 4.2 Google AI LangChain Llama Index Databricks No No Yes Partial Partial Partial 4.2 Note: Partial scores relate either to situations where you “need to build/maintain it” or “you can by integrating other parts of the stack, e.g. “Azure Search + Cosmos”. *score: 0.74707174, doc-id: RAG Options.docx*\n",
       " 8. Trust The scoring metric here was applied in relation to how much you trust the Vendor to provide reliable responses for both the retrieval and summarisation phases of the RAG pipeline. If the onus is entirely on the client to perform their own evaluation, this scores poorly amongst “application builders” who just want something that works … and works well. <b>Though Vectara scored well (I mean, yes we’re a bit biased) we still didn’t put a 10 as the market vertical is still too new – maybe after another year of great results and expanded use of Boomerang we’ll self-report a 9.</b> Advanced RAG Features What additional features doe the vendor support specifically for RAG? Result Summary Vendor Category Completeness Abstraction Team Costs Trust Adv Features Vectara SaaS (10) 10.0 Corpus (10) Low (10) 8 10 Cohere PaaS(5) 3.3 Low Level (6) Mid (5) 3 1 OpenAI SaaS(10) 9.2 Assistant (7) Mid (5) 6 3 Azure Search PaaS(5) 4.2 3 High (2) 5 2 Google AI LangChain Llama Index Databricks PaaS(5) 4.2 Low Level (4) High (2) 5 3 Databricks does a little better here with lineage from Unity Catalog lineage and Model Monitoring – two of the best features I was sad to leave behind. *score: 0.74091953, doc-id: RAG Options.docx*\n",
       " 9. <b>RAG Options What the thing is: Choosing a RAG in today’s market Options · Cohere · OpenAI · Azure Search · Google AI · LangChain · Llama Index · Databricks Summarise · Background Sentence · Who each tech is created for (App Builders | Data Engineers | DSs) · Cost model (scale to zero) · Abstraction Strengths/Weaknesses · Ease of Use (right abstraction etc) · Operational Cost · Trust · Model Coupling · Advanced Features (Hybrid) Conclusion Introduction Wow what a difference a day makes!!</b> When Vectara was founded the Retrieval Augmented Generation (RAG) was a very lonely space. But from the explosion of LLMs in 2022 have also seen the number of solutions which propose a RAG element to also increase. *score: 0.71499735, doc-id: RAG Options.docx*\n",
       " 10. Open AI, whilst it has an assistant, is not aligned to a corpus abstraction and lacks expected maturity which also affects other categories. Final Scores (recalculate with missing): 1. <b>First: Vectara (58) 2.</b> Fourth: Databricks (23.2) 5. Fifth: Azure Search (21.2) Conclusion I mean, we’re tooting our own horn and came out first? *score: 0.6951151, doc-id: RAG Options.docx*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vectara.client.util import render_markdown\n",
    "from IPython.display import display, Markdown\n",
    "rendered = render_markdown(query, response)\n",
    "display(Markdown(rendered))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
